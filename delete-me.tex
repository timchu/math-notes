%%% This is Tim's version of Deg Sparsify, no changes were made except n-1/m on
%% the thresholding of edges, and some comments are put in the very bottom as to
%% what I have left to work on.
Note that I realized that you have to treat it as a multi-graph, do order of ops
in sparsification very carefully (order of ops being like, do we first throw out
low ER, or do we first bi-partition, or what?). Then I realized that you have to
use mhat as the threshold and not m, or else the error compounds in a silly way.
So the graph should be seen as a multi-graph (of edges of length powers of 2) with the appropriate number of edges.

I forgot what else I realized today, but I think it was carefully using mhat
instead of m to do edge reduction as well, defining the terms
appropriately,labeling the steps of the algorithm so that I had an easy pointer
to them, also blatantly making stuff up and not rigorously showing why edge
count after unweightedsparsify drops by a lot but just saying that th result is just 1 and 2 weightd
edges. So that's outsourcing computation to a reader, after giving them enough
information to make it all work out. I Also handwaved the compounding error
part, which may or may not cause me problems down the line.

I think the most important part is what's easy and doable by th reader, and
what's actually hard (order of ops makes a huge difference, also knowing that it
makes a huge difference in the analysis, and also using mhat instead of m
throughout and treating it explicitly like a multigraph, only using m < mhat for
convenience. That's the only place we really ever use m.)

That's what I learned as of March 25, 2018.
\section{Degree-Preserving Spectral Sparsifiers}
\label{sec:DegreePreserving}

In this section, we describe a sparsification algorithm that involves flipping a
fair coin on each cycle with high total effective resistance, to determine
whether or not we keep it in the sparsifier. We iterate this about
$\log(\frac{m}{n})$ times to obtain a sparsifier.

\begin{theorem}
  \label{thm:DegreePreservingSparsify}
  There exists a routine $\textsc{DegreePreservingSparsify}$ that when given a
  graph $G$ with $n$ vertices and $m$ edges, as well as a cycle decomposition
  routine $\textsc{CycleDecompose}$ that produces a $k(\hat{n}), L(\hat{n})$
  short-cycle decomposition of a graph with $\hat(m)$ edges and $\hat{n}$
  vertices in $T(m,n)$ time, returns $H$ with at most

  \[
    k\left(\hat{n} \right) \log n
    + 
    O(\left(n L \left(\hat{n}\right)\right \frac{\log n}{\eps^{2}}) 
    \]
  edges in $T(m \log{n}, n) + O(m \log^2{n} \epsilon^{-2}$ time such that with
  high probability
  \begin{enumerate}
    \item $\LL_{G} \approx_{\epsilon} \LL_{H}$, and
    \item all degrees in $G$ and $H$ are the same.
  \end{enumerate}
\end{theorem}

Pseudocode for this algorithm is in
Algorithm~\ref{alg:DegreePreservingSparsify}.

\begin{algorithm}
  \caption{$\textsc{DegreePresrevingSparsify}(G, \epsilon,
  \textsc{CycleDecompose})$}.
  \begin{enumerate}
    \item While $|E(G) \geq \log{n} k(n)
      + 
      O\left( n L(n) \frac{\log n}{\eps^{-2}} \right)$
      \begin{enumerate}
        \item Compute $2$-approximate effective resistances,
          $rrtil$ for all edges of $G$
        \item $G_1, G_2, \ldots \leftarrow \textsc{UnweightedDecompose}(G)$
        \item Initialize $H \leftarrow (V, \emptyset)$.
        \item For each $G_i$
          \begin{enumerate}
            \item Let \bar{G_i} denote all edges in $G_i$ with 
              \[
                ww_e \rrtil_e \geq \frac{n-1}{C_S m}
                \]
            \item Let $H_i$ be the bipartition of $G_i \setminus \bar_{G}_i$
              with at least half of its edges.
            \item $H \leftarrow H \cup (G_i \setminus H_i)$
            \item $\{C_{i1}, C_{i2}, \ldots C_{it} \}
              \leftarrow \textsc{CycleDecompose}(H_{i})$,
              where $H_i$ is treated as an unweighted graph because all edges in
              it have the same edge weights.

            \item $H \leftarow H \setminus C_{i1} \setminus C_{i2} \ldots
              setminus C_{it}$.
            \item For each cycle $C_{ij}$
              \begin{enumerate}
                \item With probability $\frac{1}{2}$, add all the odd indexed
                  edges of $C_{ij}$ into $H$ with weights doubled,
                  otherwise add all the even indexed edges into $H$ with weights
                  doubled.
              \end{enumerate}
          \end{enumerate}
        \item $G \leftarrow H$ \tim{Check this. I think this is a mistake! Do
          you really want to be redoing the cycle decomp each step? I guess
          you can.}
    \end{enumerate}
      \item Return $G$.
    \end{enumerate}
\end{algorithm}
\tim{This is the algorithm. Things to be changed: $C_s$ needs to be defined,
make it known that we are not re-doing the full UnweightedDecompose algorithm.
Define an intermediary or an artifact: you are given buckets with edge weights
in them, and show how to update said buckets. Note that poly(n) bounded edges, I
can see giving a constant error. I don't see how it gives a poly n error though.
This seems to need $log(n) / log(n+1/n)$ buckets, which is huge. WAIT THIS IS
FINE JUST TAKE THE FIRST LOG N BITS AND EVERYTHING IS FINE>, YOU NEED INTEGER
WEIGHTS THO.}


\todo{I didn't change this part yet}

%    \tim{This is a very very rough shoddy writeup. The key part is that I
%    believe you only pick up an $l$ factor in space.  Note that I think you can
%    do better, in that I think you can pick up some sublinear function of $l$
%    because I naively bounded the effective resistance of a cycle to be the sum
%    of the effective resistances of the edges, and also did some matrix absolute
%    value shenanigans under the hood when bounding the variance parameter. Both
%    if these assumptions seem a little silly.}


\begin{proof}(Of Theorem~\ref{thm:DegreePreservingSparsify})
  The core of this proof is showing lemma~\ref{lem:GraphSampling}. This is a
  result using standard Matrix Chernoff/Matrix Bernstein techniques. This will show that
  algorithm~\ref{alg:DegreePreservingSparsify} works on unweighted graphs.
  \tim{Quantify what this means.} Then we prove lemma~\ref{lem:Reduction}, and
  prove how this shows theorem~\ref{thm:DegreePreservingSparsify}.


  % Proving lemma ~\ref{lem:GraphSampling}

  Then, we will prove
  lemma~\ref{lem:Reduction}, and 
  First, we prove lemma 

\todo{fix this proof}

    Flipping coins on cycles instead. This changes nothing except the effective resistance calculation (and now
    we sparsify down to $\frac{3}{4}$ as many edges, in each step of te
    algorithm, which ultimately changes nothing).

    This throws in an extra $l$ factor into the variance squared,
    \tim{not an $l^2$ factor, which is what I initially expected}.

    Therefore, $t$ is now $= O\left(\sqrt{\frac{nl \log n}{m}}\right)$ (and $t
    \leq 1$), and so the final number of edges $m'$ we can sparsify to while
    incurring an $\epsilon^2$ error, satisfies:

    \begin{align}
       \eps = O\left(\sqrt{\frac{nl \log n}{m'}}\right)
    \end{align}
    or
    \begin{align}
       \eps^2 = O\left(\frac{nl \log n}{m'}\right)
    \end{align}
    or
    \begin{align}
       m' = O\left(\frac{nl \log n}{\eps^2}\right)
    \end{align}
    The runtime is more than linear in $m$, so since the number of edges
    decreases geometrically at each iteration of our coin-tossing algorithm,
    the total run time is a constant factor of the runtime of the first iteration.

    That is to say, the total run time is a constant multiple of the time it takes
    to run a short cycle decomposition and estimate effective resistances on each cycle.
\end{proof}

\richard{this proof looks good to me.}

    \tim{I gave some thought into how degree preserving property might help us
    get a better sparsfier than the naive $l$ factor in space we pick up,
    where $l$ is the length of the short cycles in a short-cycle decomposition.
    I had no idea how to proceed, for a couple good reasons I will not go into here.}
   


\richard{I think this can all goto appendix?}

\subsection{Correctness of Correlated Sampling Low ER Edges}
\label{subsec:Sparsify}

\begin{lemma} \label{lem:edge-keeps}
At least $\frac{1}{2}$ of the edges in the graph have effective resistance less than $\frac{2(n-1)}{m}$. \end{lemma}

\begin{proof} The average effective resistance of edges in a graph is $\frac{n-1}{m}$, via trace arguments. The rest follows from Markov's inequality. \end{proof}

  First, we state Matrix Bernstein, and a direct lemma of Matrix Bernstein \tim{which may or may not be Matrix Azuma.} 
  \begin{theorem} (Matrix Bernstein)
    Let $X = \sum_{i=1}^s S_i$. The norm of the total variance of $X$ is:
    \begin{align}
      \sigma^2 = \norm{\sum_{i=1}^s \expec{S_i} {S_i^2}}
    \end{align}
    Then $\prob{X}{\norm{X} \geq t} \leq d \cdot e^{\frac{-t^2}{\sigma^2 + Lt/3}}$
    where $d$ is the dimension of $X$ and $L = \max \norm{S_i}$. Here,
    $\norm{M}$ represents the spectral norm of $M$. \sushant{Timothy,
      note the use of the \textbackslash norm command}

    (\tim{Check if d is defined the right way})
    %% \tim{Dimension of X? Can't X be low dimension even if the S are in high dimension?}
  \end{theorem}
  \begin{lemma} \label{lem:matrix-azuma}
    Let $X = \sum_{i=1}^s S_i$ and $S_i := \mu_i M_i$ where $M_i$ is a fixed PSd matrix and
    $\mu_i$ is a Rademacher random variable, 
    then $\prob{}{\norm{X} \geq t} \leq d \cdot e^{\frac{-t^2}{L \cdot \expec{S_i}{X}+ Lt/3}}$
    where $d$ is the dimension of $X$ and $L = max||S_i||$.
    Here, $||Y ||$ represents the spectral norm of $Y$.
  \end{lemma}
  \sushant{How does this expression have a matrix on the right in
      the denominator}
  \begin{proof} This follows from
    \begin{align}
    \expec{S_i}{S_i^2} \preceq \max_{i}||M_i|| \sum{M_i} 
    \end{align}
    when $M_i \succeq 0$.

    \tim{I'd much rather define a matrix absolute value and use $abs(S_i)$
    rather than $M_i$}
  \end{proof}

  \begin{lemma} Let $G'$ be the output of the algorithm stated in
  \ref{subsubsection:alg-desc}. Then $G'$ is with $\frac{1}{n^{O(1)}}$ probability an $1+O\left(\sqrt{\frac{n \log n}{m}}\right)$ sparsifier, with high probability. \end{lemma}
    \begin{proof}
      Consider the Laplacian $L_e$ corresponding to each edge of the graph $G$. Let
      $E'$ be the set of edges in $G$ with effective resistance less than
      $\frac{2(n-1)}{m}$.
      
      Let $S_e$ be the set of independent random variables such that: it is
      $L_e$ with probability $\frac{1}{2}$ and $-L_e$ with probability
      $\frac{1}{2}$. Recall $G'$ is the output of the algorithm in
      \ref{subsubsection:alg-desc}. Then,
      \begin{align}
        L_{G'} := \left(\sum_{e \in E} L_e \right) + \left(\sum_{e \in E'}
      S_e\right).
      \end{align}

      by construction. If we left and right multiply this equation by $L_G^{\dag/2}$ and
      substitute $X_{H} := L_G^{\dag/2} L_{H} L_G^{\dag/2}$ for any graph $H$,
      and let $T_e := L_G^{\dag/2} S_{e} L_G^{\dag/2}$ then we
      get: 

      \begin{align}
      X_{G'} := \left(\sum_{e \in E} X_e\right) + \left(\sum_{e \in E'}
      T_e\right).
      \end{align}

      Note that $\expec{T_e}{X_{G'}} = I$ and $T_e$ are independent random
      variables with expected value zero satisfying (by construction):

      \begin{align}
      ||T_e|| \leq \frac{2(n-1)}{m}
      \end{align}
      and 


      \begin{align}
      \sum_{e \in E'} L_e \preceq I
      \end{align}

      then we can apply lemma \ref{lem:matrix-azuma} to get:

      \begin{align}
      \prob{}{||X_{G'} || \geq t} \leq n \cdot
      e^{\frac{-t^2}{\frac{2(n-1)}{m} + \frac{2(n-1)t}{m}}}
      \end{align}

      %% Note that the number of independent random variables we summed didn't
      %% actually matter one iota for matrix variance -- the important part is that
      %% T_e's sum \preceq I. 

      Note that if $t = O\left(\sqrt{\frac{n \log n}{m}}\right)$ (and $t \leq 1$), then 

      \begin{align}
      n \cdot e^{\frac{-t^2}{\frac{2(n-1)}{m} + \frac{2(n-1)t}{m}}}
      = \frac{1}{n^{O(1)}} 
      \end{align}


    thereby proving that 
      
      \begin{align}
      ||X_{G'} - I|| \leq O\left(\sqrt{\frac{n\log n}{m}}\right)
      \end{align}
        with polynomially high probability.
      which proves that $L_{G'}$ is a $1 + O\left(\sqrt{\frac{n\log n}{m}}\right)$
      sparsifier of $L_G$ with
      high probability.
    \end{proof}
    \tim{I will do a major cleanup on this part...}
    Some words about how chaining this lemma we just proved, with log n
    iteration, causes error to blow up geometrically each time, with factor
    $\sqrt{2}$ which is fine, 
    making the total error $\epsilon$ thanks to convergence of geometric series.

    Likewise, the run time of a single iteration of the algorithm described in
    \ref{subsubsection:alg-desc} is also nearly linear in $m$, and is thus
    dominated by the first iteration (as edges decreases by a $\frac{1}{2}$
    factor each time.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
