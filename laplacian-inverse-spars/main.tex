\input{header}
\usepackage{float}
\begin{document}
\section{Notes by Tim on Directly Sparsifying $L_G^\dag$ via cut metrics
and tree metrics}
\begin{theorem}\label{thm:tree} Effective Resistance embeds
  isometrically into a distribution over
Tree Metrics \end{theorem}
\begin{theorem}\label{thm:cut} Effective Resistance embeds isometrically
 into a distribution over cut
metrics \end{theorem}
\begin{theorem}\label{thm:lowcut} There exists a set of $O(n \log n /
  \eps^2)$ cut metrics, whose weighted sum $(1+\eps)$ approximates effective resistance.
\end{theorem}

The first two theorems are equivalent to Effective Resistance being
located inside $L_1$, which is known in the literature. 

The third theorem is also implied by literature, since $L_1$ metrics can
all be sparsified by $O(n / \eps^2)$ cut metrics. This is not obvious,
and was a paper in 2010.

In this note, we try to write $L^\dag$ as a positive linear combination of
rank-one matrices associated to
cuts (we will call these rank-one matrices \textbf{cut matrices}, to be
defined later). This is how we will prove Theorems~\ref{thm:tree}
and~\ref{thm:cut}. Running matrix chernoff on these rank-one matrices
will lead us to Theorem~\ref{thm:cut}.

I do not claim credit for the proof of Theorem~\ref{thm:tree} or
Theorem~\ref{thm:cut}, as they were known in the literature before.

We also prove, using rather naive methods, that
\begin{conjecture}\label{thm:hightree} There exists a set of
  $O( \log n / \eps^4)$ tree
  metrics, whose sum $(1+\eps)$ approximates effective resistance.
\end{conjecture}
This shows that Effective Resistance can be written as a small number of
tree metrics, a result that was previously not known. However, this is
NOT TRUE for cycles, and I have no reason to believe it will be true for
any modification of this process! :D

We now turn our attention to a few open-ended conjectures:
\begin{conjecture}\label{conj:lowtree} There exists a set of $O( \log n / \eps^2)$ tree
  metrics, whose sum $(1+\eps)$ approximates effective resistance.
\end{conjecture}

\begin{conjecture}\label{thm:super-lowcut} There exists a set of $O(n \log n /
  \eps)$ cut metrics, whose sum $(1+\eps)$ approximates effective resistance.
\end{conjecture}
\begin{conjecture}\label{conj:super-lowtree} There exists a set of $O( \log n / \eps)$ tree
  metrics, whose sum $(1+\eps)$ approximates effective resistance.
\end{conjecture}

Conjecture~\ref{conj:lowtree} would show that effective resistance has a structural
result that it is approximately a small number of tree metrics. 

It would be nice if I had fast algorithms to do any of these, but I
don't. 
% I also suspect Conjecture~\ref{conj:lowtree} is false, but it's
% difficult for me to build a good framework for finding counterexamples.

\section{Refinement of Theorem~\ref{thm:tree}: Distribution of Tree
Metrics that Average to Effective-Resitance}

In this section, we explicitly find the distribution over trees such
that the expected value of $L_T^\dag$ is $L_G^\dag$. This directly implies
a distribution of trees whose expected value is effective resistance.

\begin{algorithm} [H]
  \caption{$\textsc{TreeGenerator(G)}$}
\begin{enumerate}
  \item Sample a uniformly random spanning tree in $G$.
  \item For each edge of the tree, weight it with the number of
    edges crossing the cut induced by the tree. (That is, each edge in
    the tree induces a cut: count the number of edges in $G$ that cross
    the cut. Weight each edge with that number)
\end{enumerate}
\end{algorithm}
\begin{theorem} \label{thm:tree-refine} \textsc{TreeGenerator(G)}
  generates a distribution on
  trees $T$ such that the expected value of $L_T^\dag$ is $L_G^\dag$.
\end{theorem}
%
  We only show that $\chiuv^T \expec{}{L_T^\dag} \chiuv = \chiuv^T L_G^\dag
  \chiuv$, where $\chiuv$ is the vector with $1$ at vertex $u$, $-1$ at
  $v$, and $0$ elsewhere. We claim without proof that if this holds for
  all $u, v \in V(G)$, and the nullspace of $\expec{}{L_T^\dag}$ and $L_G^\dag$
  are th same, then $\expec{L_T^\dag} = L_G^\dag$. That is, we claim
  without proof that it
  suffices to prove Lemma~\ref{lem:tree-refine}.

\begin{lemma} \label{lem:tree-refine} \textsc{TreeGenerator(G)}
  generates a distribution on
  trees $T$ such that the expected value of $\chiuv^T \expec{L_T^\dag}
  \chiuv$ is $\chiuv^T L_G^\dag \chiuv$.
\end{lemma}

\begin{proof} (of Lemma~\ref{lem:tree-refine}).
  We split into two cases:
  \begin{enumerate}
    \item \label{pf:tree-refine-case1} Edge $uv$ is in $G$.
    \item \label{pf:tree-refine-case2} Edge $uv$ is not in $G$.
  \end{enumerate}
  We first deal with Case~\ref{pf:tree-refine-case1}. 

  We WILDLY ASSUME that TreeGenerator gives the same distribution as
  CutGenerator. Now we just show CutGenerator gives the right thing.

  Split the set (Tree, edge) into equivalence classes, where: two
  (Tree, edge) objects are equivalent if and only if removing the edge
  results in the same two-tree forest. Call this equivalence class a
  megatree.

  Now, for a given megatree, it is chosen with probability $cut size /
  total-tree-count$
  and is weighted $1/cut size$. So overall, it contributes a weight of
  $1/total-tree-count$ per megatree.

  Note that for every tree $T$ that contains $uv$, there is a one to one
  correspondence to a megatree where $T$ is one of the representatives
  of the megatree.

  The expected distance from $u$ to $v$ in the megatree is exactly equal
  to the number of representatives $T$ containing $uv$, which is exactly
  equal to the effective resistance (assuming a unit weight graph,
  possibly with multi-edges).
  
  Now: The expected distance from $u$ to $v$ is therefore equal to
  exactly the probability a tree contains $uv$, which is exactly equal
  to the Effective Resistance. Note this holds even if $u$ and $v$ has
  multi-edges between them, and does NOT give leverage score.

  I don't know how to prove Case~\ref{pf:tree-refine-case2}.
\end{proof}

\begin{proof} (of Theorem~\ref{thm:tree-refine})
  Follows (without proof) from Lemma~\ref{lem:tree-refine}).
\end{proof}
\begin{proof} (of Theorem~\ref{thm:hightree}
  THERE IS A MISTAKE HERE AND THE THEOREM IS NOT TRUE (Breaks for
  cycles).
  We first take a graph with high leverage scores all around. Then the
  megatree is chosen with high probability (at least proportional to leverage
  score).  A graph with high leverage score that spectrally approximates
  the original can be obtained via Spielman Srivastava. 

  Now, each megatree containing $uv$ is sampled with probability equal
  to at least the leverage score of $uv$. (It is sampled with
  probability equal to cut-size times effective resistance of uv, and
  cut-size is at least the conductance from $u$ to $v$).

  Thus the probability any megatree is sampled is at least
  $ > O(\log n/\eps^2)$, as that is a uniform bound on leverage score in
  Spielman-Srivastava. This bound is VERY weak.

  The weight that each megatree contributes to the distance from $u$ to
  $v$ is less than $1 / cut size$, which is less than $1/c_e$.

  Therefore the weight on $uv$ is a average of a bunch of random variables,
  each of which is less than $1/c_e$, and averages to $ER$. (Now assume unit
  weight graph). Concentration is the same as if a bunch of random
  variables were each less than $1$, and averaged to leverage-score.
  Likewise, if a bunch of variables were weighted inverse lev score and
  averaged to 1.

  How many samples would we need from here? Chernoff bound tells us that
  for each $uv$, we would need something like: $\eps^2 s / L = \log n$
  where $L$
  is the maximum value of the sample (assuming a $1$ average), and $s$ is the number of samples
  taken. Then $s = L \log n / eps^2$, which equals $\log^2 n / eps^4$.

  This is a real shitty bound and I think I might have made a bunch of
  mistakes getting up to this point. Now you take the union bound over
  all pairs of points.

\end{proof}
%
\begin{corollary} \textsc{TreeGenerator(G)} generates a distribution on
  tree metrics whose expected value is the effective resistance.
\end{corollary}

\section{Refinement of Theorem~\ref{thm:cut}: Distribution of Cut
Metrics whose Average is Effective Resistance}
In this section, we find a distribution over cuts such that the expected
value of such cuts is equal to the effective resistance.

\begin{algorithm} [H]
  \caption{$\textsc{CutGenerator(G)}$}
\begin{enumerate}
  \item Sample a uniformly random spanning tree in $G$.
  \item Pick an edge uniformly at random from the tree. This will induce
    a cut.
  \item Weight the cut with the inverse of the number of edges crossing the
    cut, times $(n-1)$.
\end{enumerate}
\end{algorithm}
\begin{theorem}\label{thm:cut-refine} $\textsc{CutGenerator(G)}$ generates a distribution on
  cuts such that the expected value of these cuts is equivalent to
  effective resistance.
\end{theorem}

This follows directly from Theorem~\ref{thm:tree-refine}, by splitting
each tree metric into a cut metric. However, we'd like a matrix version of
Theorem~\ref{thm:cut-refine}, so we can apply Matrix Chernoff to it and
subsample a small number of cuts.

To do this, we want to find a rank one matrix associated to each cut. 
\begin{definition}\label{def:cut-matrix}
Let $G$ have $n$ vertices.  For each cut $C$ that divides the graph into vertex set $S$ and $T$,
  define a vector $u_C \in \mathbb{R}^n$ such that 
  \[ u_C(v) = |T|/n \] 
  if $v \in S$, and 
  \[ u_C(v) = -|S|/n \] 
  if $v \in T$.

For cut $C$, define its \textbf{cut matrix} $M_C$ to equal $u_C
  u_C^T$. Note that $M_C$ is orthogonal to the all-ones vector, and has
  rank one.
\end{definition}

\begin{lemma}
Let $\chi_{ij}$ equal the vector with $1$ at $i$
  and $-1$ at $j$ and $0$ everywhere else. Then $\chi_{ij}^T M_C
  \chi_{ij}$ is equal to the distance between $i$ and $j$ in the cut
  metric.
\end{lemma}
We state the above lemma without proof, and I believe it is a
straightforward verification.

\begin{theorem}
  \textsc{CutGenerator(G)} generates a distribution on cuts $C$ such
  that the expected value of $M_C$ is $L_G^\dag$.
\end{theorem}
This follows directly from theorem~\ref{thm:tree-refine}. Here, we need
to check that the sum of matrices $M_C$, where $C$ ranges over all cuts induced
by removing an edge from $T$, is equal to $L_T^\dag$. This exercise is
left to the reader :).
\section{Matrix Chernoff for $M_C$, and 'Proof' of
Theorem~\ref{thm:lowcut}}

Here, we run Matrix Chernoff on cut matrices $M_C$. I'll just state a
couple of results without proof.

Recall that in Spielman Srivastava, it is proven that if we have a
distribution of matrices weighted by the leverage score, which is
conductance times effective resistance in the case of sparsifying $L_G$,
and we weight each matrix $L_e$ by $(n-1)$ times the inverse effective resistance,
then we get a distribution over $L_e$, such that sampling and scaling
$n \log n / \eps^2$ samples from this distribution gives a $(1+\eps)$
approximation of $L_G$.

Now, we show the analog of this process when we write $L_G^\dag = \sum_C a_C
M_C$.  Here, $a_C$ turns out to be the number of two-tree spanning
forests with no edge crossing cut $C$, divided by the total number of
spanning trees of $G$. This is a known result from
folklore, or can alternately be derived from
Theorem~\ref{thm:cut-refine}.

\subsection{Analog of $c_e$}
Recall $\sum_e c_e L_e = L_G$, and analagously $\sum_C a_C M_C =
L_G^\dag$. Thus, we can loosely say that $a_C$ is the analog of $c_e$.
\subsection{Analog of Effective Resistance}
Recall that Effective Resistance in Spielman-Srivastava equals:

\[ \tr(L_G^{\dag/2} L_e L_G^{\dag/2}) \]

The analog of this for cut matrices, therefore, would be:

\[ \tr(L_G^{1/2} M_C L_G^{1/2} )\]
\[ \tr(L_G^{1/2} u_Cu_C^T L_G^{1/2}) \]
\[ = \tr(u_C^T L_G u_C )\]
\[ = u_C^T L_G u_C, \]
where $u_C$ was defined in Definition~\ref{def:cut-matrix}.  Here, it is not
too difficult to show that this quantity equals the number of edges
in $G$ crossing cut $C$.

\subsection{Analog of Leverage Score}
The analog of conductance times effective resistance, is $a_C \cdot
u_C^T L_G u_C$, which turns out to equal the number of spanning trees
with exactly one edge crossing cut $C$.

I haven't rigorously worked through the analogies above, and this is a
crucial step I haven't done yet.

\begin{theorem} Sampling and rescaling $n \log n / \eps^2$
  values of $M_C$ via \textsc{CutGenerator(G)}, and adding the results,
  gives us $L_G^\dag$ within $(1+\eps)$ approximate factor, with high
  probability.
\end{theorem}
The above theorem follows from our computation of leverage score,
Matrix Chernoff, and
the way CutGenerator is defined. I have omitted details in the
proof, and these details certainly need checking. 

This proves Theorem~\ref{thm:lowcut}, and gives a sampling method for
generating an empirical sample of cut metrics whose average is effective
resistance.

\section{Open Ended Questions}
In CutGenerator, you pick a random spanning tree and sample
each edge with $\frac{1}{n-1}$ probability to get a cut. If you just
took the batch of $(n-1)$ cuts associated with the tree, you would get
TreeGenerator. We need $ n \log n / \eps^2$ cuts from CutGenerator to
approximate $L_G^\dag$ with high probability.
Perhaps we might only need $\log n / \eps^2$ trees from TreeGenerator?
This gives us Conjecture~\ref{conj:lowtree}.  
Sadly, I have no idea how to proceed with this! I would believe it is
false, but it is unclear how to systematically provide counter-examples.

Incidentally, I believe we can use the above framework to
embed any $n$-item metric in $l_1$, into $l_1^{n \log n / \eps^2}$
with $(1+\eps)$ distortion. It's possible we can get rid of $\eps^2$ and
replace it with $\eps$, kind of like how JS'17 does it.

Note that in general, $l_1$ isometrically embeds into $l_1^{n(n-1)/2}$,
and $n(n-1)/2$ is tight.
\end{document}
