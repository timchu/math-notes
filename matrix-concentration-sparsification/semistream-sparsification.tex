\input{header}
\begin{document}

Note: Tim no longer thinks this works. Each edge exists with probability 
\[ \frac{w_e r_e(H_t)}{blah} \]
  times some $\log n \eps^{-2}$ factor. Howver, each edge is not an
  independent coin-flip.
We attempt to recreate semi-streaming sparsification (knowing now that
    it's possible) without the general framework of resparsification.

Note on Spielman (regular):
  (No weights) Using per-edge Spielman Srivastava, we know we want $n \log n
  \eps^{-2}$ edges in the end, so we keep edges with probability
  $\frac{r_e \log n}{\eps^2}$. (Reweighting is the reciprocal of this
      probability.)

Therefore in resparsification, we would (ideally) like to not lose edges
that are chosen with probabilty $1$, which is equivalent to not losing
edges with
\begin{align}
& \frac{r_e \log n}{\eps^2}  > 1
\\ 
\end{align}


When the first $2T$ edges of the semi-stream is graph $H$, then our algorithm: flips coins
on edges with $\frac{r_e(H) \log n}{\eps^2} < 1$ with probability
equal to that number. Otherwise it keeps the edge in by default. 

Here, $T = \frac{n \log n}{\eps^2}$.

\begin{lemma}

No edges with $\frac{r_e(G) \log n}{\eps^2} > 1$ are
perturbed by the above algorithm
\end{lemma}

Perturbed edges in the algorithm are edges where $\frac{r_e(H) \log n}{\eps^2} < 1$.
We show that any edge with this property also satisfies the same
property for $G$.
\begin{proof}
Note that all edges $e$ with 
\begin{align*}
\frac{r_e(H) \log n}{\eps^2} < 1
\end{align*}

also satisfy:

\begin{align*}
& \frac{r_e(G) \log n}{\eps^2}
\\
& \leq \frac{r_e(H) \log n}{\eps^2}
\\ 
&< 1
\end{align*}
because for any subset $H \subset G$,

$r_e(G) \leq r_e(H)$.

Therefore, no perturbed edges has $\frac{r_e(G) \log n}{\eps^2} > 1$.

\end{proof}
\section{Semi-Streaming sparsification without Predictable Quadratic
  Variance}

Now we prove that semi-streaming sparisification as described in the
resparsification game paper of KPPS works, but without using predictable
quadratic variation. All we really do is show that the algorithm in
$KPPS$ zeros out an edge with low probability, enough to satisfy the
guarantees of Spielman Srivastava sparsification.

To do this, we will use two lemmas and an induction argument.

\begin{lemma}
For $H \subset G$, we have $r_e(H) \leq r_e(G)$.
\end{lemma}

\begin{lemma}
(Known variation on Spielman Srivastava).
For any graph $G$, suppose $\rr_e$ is a $2$-approximation on the
effective resistance $r_e$ in $G$. Let $w_e$ denote the weight of edge
$e$. For each edge $e$, flip a coin with probability :

\begin{align}
C \cdot \frac{w_e \rr_e \log n}{\eps^2}
\end{align}

and with that probability weight the edge with weight

\begin{align}
\frac{\eps^2}{\rr_e \log n}
\end{align}

then the resulting graph is a $1+\eps$-sparsifier of $G$ with high probability,
     for some constant $C$. The number of edges in this graph is roughly
     $C n \log n \eps^{-2}$ edges.
\end{lemma}

We outline the semi-streaming algorithm used in this note,  which is essentially identical
to the one in KPPS. Let $T$ be $2C \cdot \left(\frac{n \log n}{\eps^2}\right)$.  
Let $G_k$ be the subgraph formed by the first $k$ edges of our stream.
Let the $k^{th}$ edge in the stream be $E_k$.
(Here, $G_m =:G$, where $G$ is the graph formed by all edges of the
 stream).

The algorithm is:
%% SEMI-STREAM ALGORITHM DESCRIPTION

\begin{algorithm}
  \caption{$\textsc{SemistreamSparsify}(G, \eps)$}
\textbf{Input:} Positively weighted graph $G$ with $m$ edges adn $n$ vertices.

\begin{enumerate}
\item While $k < m$:
\begin{enumerate}
  \item $H_k \leftarrow H_{k-1} + E_k$.
  \item If $|E(H_k)| > 2T$:
    \begin{enumerate}
    \item Flip a coin on each edge with probability 
    $C \cdot \frac{w_e r_e(H_k) \log n}{\eps^2}$ 
    and weight it with: $\frac{\eps^2}{r_e(H_k)}$.
    \label{alg:step:resparsification}
    \end{enumerate}
  \item $H_k$ is the output of the above procedure.
\end{enumerate}
\end{enumerate}
\label{alg:semi-stream}
\end{algorithm}

This gets you under $T$ edges with high probability. We will prove a few
lemmas about this algorithm inductively, which will show its a good
sparsifier with high probability.

%% INDUCTIVE LEMMA
\begin{lemma}
\label{lem:inductive}
$H_{k-1}$ as defined in the algorithm is a good $1+\eps$ sparsifier of
$G_k$ for all $k$.
\end{lemma}
We will prove this lemma by induction. For this, we need:

% BOUND ON PROBABILITY EACH EDGE IS NONZERO

\begin{lemma}
\label{lem:ER}
The probability edge $e:=E_{k'}$ is non-zero at time $k > k'$ is bounded
below
by:
\begin{align}
(1-\eps) \frac{w_e r_e(H_k) \log n}{\eps^2}
\end{align}
\end{lemma}

IF $H_k = H_{k-1} + E_k$, then Lemma~\ref{lem:inductive} follows from
$H_{k-1}$ being a good sparsifier of $G_{k-1}$.

If $H_k$ undergoes a re-sparsification step
(step~\ref{alg:step:resparsification} in \textsc{SemistreamSparsify}
 then the probability any edge $e:=E_{k'}$ is non-zero in the final value
 of $H_k$, for $k' \leq k$, is
 equal to

\begin{align}
\prod_{t \in T_{k', k}}  \left(w_{e, t} \cdot r_e(H_t) \cdot \frac{\log
n}{\eps^2}\right)
\label{eq:Product}
\end{align}
where $T_{k', k}$ represents the set of times $t \leq k$ where $E_{k'}$ was
sparsified. Here, $w_{e,t}$ is the weight of edge $t$ in $H_t$, and
$r_e(H_t)$ is the effective resistance of $H_t$.

However, note that $w_{e,t} = w_e$ if $t$ is the first time $e$ is
resparsified, and $\frac{\eps^2}{r_e(H_{t-1})}$ otherwise. Therefore,
the whole product in Equation~\eqref{eq:Product} telescopes into:

\begin{align}
w_e r_e(H_{t'}) \frac{\log n}{\eps^2}
\label{eq:SimplifiedProduct}
\end{align}
where $t'$ is the last time $E_{k'}$ was sparsified before time $k$.
If $H_{t'}$ is a spectral sparsifier of $G_{t'}$, then we're done.
However, this is true because ;q

Oops this doens't work because all I did was bound the probability an
edge was picked, but I did not successfully show the edge picks are
independent! (And indeed they are not!)


\end{document}
